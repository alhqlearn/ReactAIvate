{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd64deb7-ae0f-413d-9a68-dc916fdb0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from dgllife.model import model_zoo\n",
    "from dgllife.utils import smiles_to_bigraph\n",
    "from dgllife.utils import EarlyStopping, Meter\n",
    "from dgllife.utils import AttentiveFPAtomFeaturizer\n",
    "from dgllife.utils import AttentiveFPBondFeaturizer\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from IPython.display import SVG, display\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "import argparse\n",
    "from rdkit import RDLogger \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RDLogger.DisableLog('rdApp.*') # switch off RDKit warning messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c5f86-3ee9-4017-af62-0dc30248c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_values_at_positions, atom_finder, smiles_augmentation, concat_feature_reactive_atom, collate_molgraphs, Canon_SMILES_similarity\n",
    "from model import AttentiveFPPredictor_rxn, weighted_binary_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44e043-3364-4093-8280-b4e714030ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_featurizer = AttentiveFPAtomFeaturizer(atom_data_field='hv')\n",
    "bond_featurizer = AttentiveFPBondFeaturizer(bond_data_field='he')\n",
    "n_feats = atom_featurizer.feat_size('hv')\n",
    "e_feats = bond_featurizer.feat_size('he')\n",
    "print( 'Number of features in graph : ' , n_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7149fd03-67e1-48a7-88d3-d995f2031e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign device \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2367f09c-e744-45b2-be49-cde0c29ca20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elementary = pd.read_csv('elementary_step_100000.csv')\n",
    "df_elementary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316d536b-4113-4902-99b6-12df80613a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets_, test_datasets = train_test_split( df_elementary, test_size=0.2, random_state=42, shuffle = False)\n",
    "train_datasets, valid_datasets = train_test_split( train_datasets_, test_size=0.125, random_state=42, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37868a08-f674-40f9-b856-56c0134f74d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datasets = valid_datasets.reset_index(drop=True)\n",
    "test_datasets = test_datasets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec6850-8a36-47c9-8ee0-08f759a8e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augm_smiles = smiles_augmentation(train_datasets, 5, augmentation =False)\n",
    "valid_augm_smiles = smiles_augmentation(valid_datasets, 5, augmentation =False)\n",
    "test_augm_smiles = smiles_augmentation(test_datasets, 5, augmentation =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f16c6e-9bce-4651-9dfb-3bcd4f237b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ( 'Total number of reaction steps after SMILES augmentation : ', len(train_augm_smiles) + len(valid_augm_smiles)+ len(test_augm_smiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5c13e-0aa3-469c-9ea0-601744050ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_generation(df_augm_smiles):    \n",
    "    graph_for_rxn = []\n",
    "    for i in range(len(df_augm_smiles)):\n",
    "        graph_for_rxn.append(smiles_to_bigraph(df_augm_smiles[i][0], node_featurizer=atom_featurizer,edge_featurizer=bond_featurizer, canonical_atom_order=False))\n",
    "    return graph_for_rxn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739fdc8a-c345-4e67-9bd9-756b8773008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_for_rxn = graph_generation(train_augm_smiles)\n",
    "valid_graph_for_rxn = graph_generation(valid_augm_smiles)\n",
    "test_graph_for_rxn = graph_generation(test_augm_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463430e4-a0ba-4284-baa7-4b0f9778d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_dataset = concat_feature_reactive_atom(train_graph_for_rxn, train_augm_smiles)\n",
    "valid_graph_dataset = concat_feature_reactive_atom(valid_graph_for_rxn, valid_augm_smiles)\n",
    "test_graph_dataset = concat_feature_reactive_atom(test_graph_for_rxn, test_augm_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e5c2c-e42d-45b8-8917-1f7cc60721b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_graph_dataset, batch_size=256,shuffle=False,\n",
    "                          collate_fn=collate_molgraphs)\n",
    "valid_loader = DataLoader(valid_graph_dataset, batch_size=256,shuffle=False,\n",
    "                          collate_fn=collate_molgraphs)\n",
    "test_loader = DataLoader(test_graph_dataset, batch_size=256,shuffle=False,\n",
    "                          collate_fn=collate_molgraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630422cc-b1d0-441b-bb6b-61d76626ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out of distribution dataloader preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee7f474-8f14-4c5e-a9ea-d3b6d887c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ood_1000 = pd.read_csv('OOD_elementary_step_3647.csv')\n",
    "print(df_ood_1000)\n",
    "ood_augm_smiles = smiles_augmentation(df_ood_1000, 5, augmentation =False)\n",
    "ood_graph_for_rxn = graph_generation(ood_augm_smiles)\n",
    "ood_graph_dataset = concat_feature_reactive_atom(ood_graph_for_rxn, ood_augm_smiles)\n",
    "ood_loader = DataLoader(ood_graph_dataset, batch_size=256,shuffle=False,\n",
    "                          collate_fn=collate_molgraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbfba61-4b9b-4409-a45a-5f682645a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the model to fit your classification task\n",
    "model = AttentiveFPPredictor_rxn(node_feat_size=n_feats,\n",
    "                                   edge_feat_size=e_feats,\n",
    "                                   num_layers=2,\n",
    "                                   num_timesteps=1,\n",
    "                                   graph_feat_size=200,\n",
    "                                   n_tasks=8,\n",
    "                                   dropout=0.1\n",
    "                                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9210e1d-bf53-4248-8d5c-aba3ee884dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca198aa3-b897-4f12-b84a-cf8f90564d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "loss_fn_graph = nn.CrossEntropyLoss()\n",
    "loss_fn_node = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60be92a-d9b2-49d9-88ee-cb0c78de7161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_a_train_epoch(n_epochs, epoch, model, data_loader, loss_criterion1, loss_criterion2, optimizer):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    loss_node_app = []\n",
    "    loss_graph_radomize_app = []\n",
    "    y_true_node = []\n",
    "    y_pred_node = []\n",
    "    \n",
    "    for batch_id, batch_data in enumerate(data_loader):\n",
    "        \n",
    "        smiles, bg, labels = batch_data\n",
    "        \n",
    "        bg = bg.to(device)\n",
    "        labels = labels.to(device)\n",
    "        n_feats_w_l = bg.ndata.pop('hv').to(device)\n",
    "        e_feats_ = bg.edata.pop('he').to(device)\n",
    "        n_feats_ = n_feats_w_l[:,:n_feats]\n",
    "        prediction1, prediction2, graph_feat = model(bg, n_feats_, e_feats_)\n",
    "        n_labels = n_feats_w_l[:,n_feats].unsqueeze(1)\n",
    "    \n",
    "        # Calculate the weights\n",
    "        counts = torch.bincount(n_labels.view(-1).long())\n",
    "        class_weights = 1.0 / counts.float()\n",
    "        class_weights = class_weights / class_weights.sum()\n",
    "    \n",
    "        \n",
    "        loss_graph = loss_fn_graph(prediction1, labels.squeeze(1).long())\n",
    "        loss_node = weighted_binary_cross_entropy(prediction2,n_labels ,class_weights)\n",
    "        loss = loss_graph + loss_node \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss_graph.data.item())\n",
    "        loss_node_app.append(loss_node.data.item())\n",
    "        \n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(prediction1.detach().cpu().numpy())\n",
    "    \n",
    "        y_true_node.extend(n_labels.cpu().numpy())\n",
    "        y_pred_node.extend(prediction2.detach().cpu().numpy())\n",
    "\n",
    "    \n",
    "    total_loss = np.mean(losses)\n",
    "    total_loss_node = np.mean(loss_node_app)\n",
    "    total_loss_graph_random = np.mean(loss_graph_radomize_app)\n",
    "    accuracy = accuracy_score(y_true, np.argmax(y_pred, axis=1))\n",
    "    print('F1 score classification task:', f1_score(y_true,np.argmax(y_pred, axis=1), average='macro'))\n",
    "\n",
    "    # Threshold for binary prediction\n",
    "    threshold_1 = 0.5\n",
    "    # Convert predicted probabilities to binary values\n",
    "    y_pred_binary = [1 if pred >= threshold_1 else 0 for pred in np.concatenate(y_pred_node)]\n",
    "    y_true_flat = np.concatenate(y_true_node)\n",
    "    # Calculate accuracy score\n",
    "    accuracy_node = accuracy_score(y_true_flat, y_pred_binary)\n",
    "    print('F1 score reactive atom task:', f1_score(y_true_flat,np.array(y_pred_binary,dtype=np.float32), average='macro'))\n",
    "\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print('epoch {:d}/{:d},train_acc_classification {:.4f},train_node_acc {:.4f},train_loss {:.4f},train_node_loss {:.4f}'.format(\n",
    "            epoch + 1, n_epochs, accuracy,accuracy_node, total_loss, total_loss_node))\n",
    "    return accuracy, total_loss, labels, prediction1, y_true_node, y_pred_node, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93145b5-ad19-4f5e-8c06-01ddeb1bab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_a_valid_epoch(n_epochs, epoch, model, data_loader, loss_criterion1, loss_criterion2):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    loss_node_app = []\n",
    "    loss_graph_radomize_app = []\n",
    "    y_true_node = []\n",
    "    y_pred_node = []\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_id, batch_data in enumerate(data_loader):\n",
    "            \n",
    "            smiles, bg, labels = batch_data\n",
    "            \n",
    "            bg = bg.to(device)\n",
    "            labels = labels.to(device)\n",
    "            n_feats_w_l = bg.ndata.pop('hv').to(device)\n",
    "            e_feats_ = bg.edata.pop('he').to(device)\n",
    "            n_feats_ = n_feats_w_l[:,:n_feats]\n",
    "            prediction1, prediction2, graph_feat = model(bg, n_feats_, e_feats_)\n",
    "            n_labels = n_feats_w_l[:,n_feats].unsqueeze(1)\n",
    "        \n",
    "            # Calculate the weights\n",
    "            counts = torch.bincount(n_labels.view(-1).long())\n",
    "            class_weights = 1.0 / counts.float()\n",
    "            class_weights = class_weights / class_weights.sum()\n",
    "            \n",
    "            loss_graph = loss_fn_graph(prediction1, labels.squeeze(1).long())\n",
    "            loss_node = weighted_binary_cross_entropy(prediction2,n_labels ,class_weights) #class_weights\n",
    "\n",
    "            loss = loss_graph + loss_node \n",
    "            \n",
    "            losses.append(loss_graph.data.item())\n",
    "            loss_node_app.append(loss_node.data.item())\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(prediction1.detach().cpu().numpy())\n",
    "        \n",
    "            y_true_node.extend(n_labels.cpu().numpy())\n",
    "            y_pred_node.extend(prediction2.detach().cpu().numpy())\n",
    "\n",
    "    \n",
    "    total_loss = np.mean(losses)\n",
    "    total_loss_node = np.mean(loss_node_app)\n",
    "    total_loss_graph_random = np.mean(loss_graph_radomize_app)\n",
    "    accuracy = accuracy_score(y_true, np.argmax(y_pred, axis=1))\n",
    "    print('F1 score classification task:', f1_score(y_true,np.argmax(y_pred, axis=1), average='macro'))\n",
    "\n",
    "    # Threshold for binary prediction\n",
    "    threshold_1 = 0.5\n",
    "    # Convert predicted probabilities to binary values\n",
    "    y_pred_binary = [1 if pred >= threshold_1 else 0 for pred in np.concatenate(y_pred_node)]\n",
    "    y_true_flat = np.concatenate(y_true_node)\n",
    "    # Calculate accuracy score\n",
    "    accuracy_node = accuracy_score(y_true_flat, y_pred_binary)\n",
    "    print('F1 score reactive atom task:', f1_score(y_true_flat,np.array(y_pred_binary,dtype=np.float32), average='macro'))\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print('epoch {:d}/{:d},valid_acc_classification {:.4f}, valid_node_acc {:.4f},valid_loss {:.4f},valid_node_loss {:.4f}'.format(\n",
    "            epoch + 1, n_epochs, accuracy, accuracy_node, total_loss, total_loss_node))\n",
    "    return accuracy, total_loss, labels, prediction1, y_true_node, y_pred_node, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371a0a84-1022-405a-8e45-1a92a0cf3da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "st_time = time.time()\n",
    "stopper = EarlyStopping(mode='higher', patience=5)\n",
    "n_epochs = 5\n",
    "for e in range(n_epochs):\n",
    "    accuracy, total_loss, labels, prediction, y_true_node, y_pred_node, train_model= run_a_train_epoch(n_epochs, e, model, train_loader, loss_fn_graph, loss_fn_node, optimizer)\n",
    "    accuracy_, total_loss_, labels_, prediction_, y_true_node_, y_pred_node_, train_model_= run_a_valid_epoch(n_epochs, e, model, valid_loader, loss_fn_graph, loss_fn_node)\n",
    "\n",
    "    #fn = 'model_' + str(e)\n",
    "        #torch.save(train_model.state_dict(), fn)\n",
    "en_time = time.time()\n",
    "print('time required:', (en_time-st_time)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4827fed0-33b2-4668-8d1b-b9771b862f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy calculation\n",
    "accuracy_, total_loss_, labels_, prediction_, y_true_node_, y_pred_node_, train_model_= run_a_valid_epoch(1, 1, model, test_loader, loss_fn_graph, loss_fn_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea46cf9-2d79-4528-b742-cb1ceecb0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOD accuracy calculation\n",
    "accuracy_, total_loss_, labels_, prediction_, y_true_node_, y_pred_node_, train_model_= run_a_valid_epoch(1, 1, model, ood_loader, loss_fn_graph, loss_fn_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58677a2a-502a-4686-8ba6-aa960fc4156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to save your model run this cell\n",
    "#fn = 'final_trained_ReactAIvate_model'\n",
    "#torch.save(model.state_dict(), fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08195a2f-afe8-43e4-9044-6c12759589bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981b7c40-3751-4e97-beca-6ce68f68ab41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871fe1a-add6-475d-af76-a482cde595a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ReactAIvate",
   "language": "python",
   "name": "reactaivate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
